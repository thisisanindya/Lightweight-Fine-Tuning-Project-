{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f502f1",
   "metadata": {},
   "source": [
    "# 1. Prepare the Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb39dfa",
   "metadata": {},
   "source": [
    " TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA [Low Rank Adaptation]\n",
    "* Model: gpt2\n",
    "* Evaluation approach: Hugging Face Training, Evaluation\n",
    "* Fine-tuning dataset: sms_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c455bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/student/.local/lib/python3.10/site-packages (2.15.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/.local/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/student/.local/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: packaging in /home/student/.local/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/student/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.15.0\n",
      "    Uninstalling datasets-2.15.0:\n",
      "      Successfully uninstalled datasets-2.15.0\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed datasets-3.4.1\n",
      "\u001b[33m  WARNING: The script datasets-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/student/.local/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "# Install the required version of datasets in case you have an older version\n",
    "# You will need to choose \"Kernel > Restart Kernel\" from the menu after executing this cell\n",
    "!pip install -U datasets\n",
    "!pip install -q \"datasets==2.15.0\"\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5d230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'output' deleted.\n",
      "'lora_output' deleted.\n"
     ]
    }
   ],
   "source": [
    "# Remove all previously create models\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "dir_names = [\"output\", \"lora_output\", \"new_output\"]\n",
    "i = 0\n",
    "for i in range(len(dir_names)-1):\n",
    "    if os.path.isdir(dir_names[i]):\n",
    "        shutil.rmtree(dir_names[i])  # Deletes the directory and its contents\n",
    "        print(f\"'{dir_names[i]}' deleted.\")\n",
    "    else:\n",
    "        print(f\"'{dir_names[i]}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad5bd8",
   "metadata": {},
   "source": [
    "## Load and Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a75a850a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sms': \"Yeah I should be able to, I'll text you when I'm ready to meet up\\n\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sms_spam dataset\n",
    "# See: https://huggingface.co/datasets/sms_spam\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
    "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=23\n",
    ")\n",
    "\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# View the dataset characteristics\n",
    "dataset[\"train\"]\n",
    "\n",
    "# Check sample data\n",
    "dataset[\"train\"][10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c60288",
   "metadata": {},
   "source": [
    "Now we are going to process our datasets by converting all the text into tokens for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae66262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 4459\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the padding token to be the EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Let's use a lambda function to tokenize all the examples\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"sms\"], padding=True,truncation=True,return_tensors=\"pt\"), batched=True\n",
    "    )\n",
    "    \n",
    "# Inspect the available columns in the dataset\n",
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbd338",
   "metadata": {},
   "source": [
    "## Load Pre-trained Hugging Face Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "952b55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"not spam\", 1: \"spam\"},\n",
    "    label2id={\"not spam\": 0, \"spam\": 1},\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Ensure the model is on CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "\n",
    "# Unfreeze all the model parameters.\n",
    "# Hint: Check the documentation at https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61c2c7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling pad_token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Resize the model embeddings to include the new token\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bdbc946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50258, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa90023",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "It's time to train our model. We'll use the Trainer class.\n",
    "\n",
    "First we'll define a function to compute our accuracy metreic then we make the Trainer.\n",
    "\n",
    "In this instance, we will fill in some of the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c398f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4460' max='4460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4460/4460 10:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.085634</td>\n",
       "      <td>0.988341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.061189</td>\n",
       "      <td>0.992825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4460, training_loss=0.10164233558381085, metrics={'train_runtime': 625.3074, 'train_samples_per_second': 14.262, 'train_steps_per_second': 7.132, 'total_flos': 882610409668608.0, 'train_loss': 0.10164233558381085, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "'''\n",
    "def compute_metrics(eval_pred):\n",
    "    print(\"eval_pred -> \", eval_pred, \"  Type ->  \", type(eval_pred))\n",
    "    \n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.tensor(predictions).to(\"cuda\")\n",
    "    labels = torch.tensor(labels).to(\"cuda\")\n",
    "    \n",
    "    print(\"predictions -> \", predictions, \"  Type ->  \", type(predictions))\n",
    "    print(\"labels -> \", labels, \"  Type ->  \", type(labels))\n",
    "    \n",
    "    #preds = eval_pred.predictions.argmax(-1)\n",
    "    #print(\"preds -> \", preds, \"  Type ->  \", type(preds))\n",
    "    \n",
    "    # predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions - labels).mean()}\n",
    "'''\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    #precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    return {\"accuracy\": accuracy}\n",
    "    \n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./output\",\n",
    "        # Set the learning rate\n",
    "         learning_rate=2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5dd85",
   "metadata": {},
   "source": [
    "## Evaluate the model \n",
    "Evaluating the model is as simple as calling the evaluate method on the trainer object. This will run the model on the test set and compute the metrics we specified in the compute_metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b5b49e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.061188604682683945,\n",
       " 'eval_accuracy': 0.9928251121076234,\n",
       " 'eval_runtime': 16.6563,\n",
       " 'eval_samples_per_second': 66.942,\n",
       " 'eval_steps_per_second': 33.501,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccfa37",
   "metadata": {},
   "source": [
    "## View the Results\n",
    "Let's look at a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ce336fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy new years melody!\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had askd u a question some hours before. Its answer\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  sms  \\\n",
       "0                                                                                  Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n   \n",
       "1                                                                                                                                           Happy new years melody!\\n   \n",
       "2                           PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n   \n",
       "3     URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n   \n",
       "4                                                                                                             I had askd u a question some hours before. Its answer\\n   \n",
       "5        SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\\n   \n",
       "6  Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\\n   \n",
       "7                     Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\\n   \n",
       "\n",
       "   predictions  labels  \n",
       "0            0       0  \n",
       "1            0       0  \n",
       "2            1       1  \n",
       "3            1       1  \n",
       "4            0       0  \n",
       "5            0       1  \n",
       "6            0       0  \n",
       "7            1       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a dataframe with the predictions and the text and the labels\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure model is on the right device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "items_for_manual_review = tokenized_dataset[\"test\"].select(\n",
    "    [0, 1, 22, 31, 43, 292, 448, 487]\n",
    ")\n",
    "\n",
    "results = trainer.predict(items_for_manual_review)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"sms\": [item[\"sms\"] for item in items_for_manual_review],\n",
    "        \"predictions\": results.predictions.argmax(axis=1),\n",
    "        \"labels\": results.label_ids,\n",
    "    }\n",
    ")\n",
    "# Show all the cell\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "# 2.Perform Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fb2f6",
   "metadata": {},
   "source": [
    "## Create a PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04c45495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Configuration Details ---> \n",
      " LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=8, target_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from peft import LoraConfig, TaskType\n",
    "'''\n",
    "config = LoraConfig( \n",
    "            task_type=TaskType.SEQ_CLS, \n",
    "            inference_mode=False, \n",
    "            r=8,\n",
    "            lora_alpha=8, \n",
    "            lora_dropout=0.1,\n",
    "            fan_in_fan_out=True\n",
    "            )\n",
    "'''\n",
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration Details ---> \\n\", lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5d0c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,984 || all params: 124,737,792 || trainable%: 0.23888830740245906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "lora_model = get_peft_model(new_model, lora_config)\n",
    "\n",
    "# Check the trainable parameters\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Check If Any Tensors Are Still on GPU\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if param.device.type != \"cpu\":\n",
    "        print(f\"Parameter {name} is on {param.device}\")\n",
    "\n",
    "# Ensure the model is on CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lora_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba334d",
   "metadata": {},
   "source": [
    "# Few Additional Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a81eca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Check for Invalid Label Index (Most Common Cause for CUDA error)\n",
    "print(set(tokenized_dataset[\"train\"][\"label\"]))\n",
    "print(set(tokenized_dataset[\"test\"][\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9212002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Check for Incorrect Model Output Shape to avoid CUDA error\n",
    "\n",
    "import torch\n",
    "for batch in tokenized_dataset[\"train\"]:\n",
    "    input_tokens = tokenizer(batch['sms'], return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        logits = lora_model(**input_tokens).logits\n",
    "        print(\"logits.shape\", logits.shape)\n",
    "        break\n",
    "        \n",
    "# Check Dataset Tensors - if any part of train_dataset is still on CUDA, it could trigger this error. Check\n",
    "for batch in tokenized_dataset[\"train\"]:\n",
    "    for key, tensor in batch.items():\n",
    "        if isinstance(tensor, torch.Tensor) and tensor.device.type != \"cpu\":\n",
    "            print(f\"Tensor {key} is on {tensor.device}\")              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "751f18f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "Moving base_model.model.transformer.wte.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.wpe.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.0.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.1.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.2.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.3.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.4.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.5.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.6.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.7.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.8.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.9.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.10.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.ln_1.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.ln_1.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.attn.c_attn.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.attn.c_attn.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.attn.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.attn.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.ln_2.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.ln_2.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.mlp.c_fc.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.mlp.c_fc.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.mlp.c_proj.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.h.11.mlp.c_proj.bias from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.ln_f.weight from cuda:0 to CPU\n",
      "Moving base_model.model.transformer.ln_f.bias from cuda:0 to CPU\n",
      "Moving base_model.model.score.original_module.weight from cuda:0 to CPU\n",
      "Moving base_model.model.score.modules_to_save.default.weight from cuda:0 to CPU\n",
      "Pad Token ID: 50256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 0: Had your mobile 10 mths? Update to the latest Camera/Video phones for FREE. KEEP UR SAME NUMBER, Get extra free mins/texts. Text YES for a call\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Text 1: Like  &lt;#&gt;, same question\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Text 2: Should I have picked up a receipt or something earlier\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Text 3: Lovely smell on this bus and it ain't tobacco... \n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "Text 4: Ok...\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Check Label Type Mismatch (Subtle Error) - to avoid CUDA error\n",
    "for batch in tokenized_dataset[\"train\"]:\n",
    "    print(type(batch[\"label\"]))\n",
    "    break\n",
    "    \n",
    "# Convert labels to int64    \n",
    "# import torch\n",
    "#tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].map(lambda x: {\"label\": torch.tensor(x[\"label\"], dtype=torch.long)})\n",
    "#tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].map(lambda x: {\"label\": torch.tensor(x[\"label\"], dtype=torch.long)})    \n",
    "#for batch in tokenized_dataset[\"train\"]:\n",
    "#    print(type(batch[\"label\"]))\n",
    "#    break\n",
    "    \n",
    "import torch\n",
    "\n",
    "# Move all LoRA layers to CPU\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if param.device.type != \"cpu\":\n",
    "        print(f\"Moving {name} from {param.device} to CPU\")\n",
    "        param.data = param.data.to(\"cpu\")\n",
    "        if param.grad is not None:\n",
    "            param.grad = param.grad.to(\"cpu\")\n",
    "\n",
    "# Move dataset to CPU\n",
    "tokenized_dataset[\"train\"].set_format(\"torch\", device=\"cpu\")\n",
    "tokenized_dataset[\"test\"].set_format(\"torch\", device=\"cpu\")\n",
    "\n",
    "# Confirm everything is on CPU \n",
    "# This forces all parameters (including LoRA layers) and dataset tensors onto CPU.\n",
    "for name, param in lora_model.named_parameters():\n",
    "    assert param.device.type == \"cpu\", f\"{name} is still on {param.device}\"\n",
    "\n",
    "# Ensure Labels Are Converted Correctly    \n",
    "#tokenized_dataset = tokenized_dataset.map(lambda x: {\"label\": int(x[\"label\"])})\n",
    "#tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Re-check Special Tokens Addition\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Safer option\n",
    "print(\"Pad Token ID:\", tokenizer.pad_token_id)\n",
    "\n",
    "# Ensure Mapping Order\n",
    "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})  \n",
    "dataset = dataset.map(lambda x: tokenizer(x[\"sms\"], padding=\"max_length\", truncation=True, max_length=128), batched=True)\n",
    "\n",
    "# Define a Padding Token for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check for Excessive Padding\n",
    "for i in range(5):\n",
    "    print(f\"Text {i}: {tokenizer.decode(tokenized_dataset['train']['input_ids'][i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd59e8e",
   "metadata": {},
   "source": [
    "## Training/fine-tuning the PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9eceec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8918' max='8918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8918/8918 07:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.120287</td>\n",
       "      <td>0.977578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.126462</td>\n",
       "      <td>0.981166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8918, training_loss=0.3575924545372781, metrics={'train_runtime': 437.4873, 'train_samples_per_second': 20.385, 'train_steps_per_second': 20.385, 'total_flos': 772381175021568.0, 'train_loss': 0.3575924545372781, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle cuda related issues\n",
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    #precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./lora_output\",\n",
    "        # Set the learning rate\n",
    "        learning_rate=2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8), # Enable Padding in the Data Collator\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01c5b2d",
   "metadata": {},
   "source": [
    "## Evaluate the lora model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba882f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12028667330741882,\n",
       " 'eval_accuracy': 0.9775784753363229,\n",
       " 'eval_runtime': 20.7917,\n",
       " 'eval_samples_per_second': 53.627,\n",
       " 'eval_steps_per_second': 53.627,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle cuda related issues\n",
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9a9b1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy new years melody!\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had askd u a question some hours before. Its answer\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  sms  \\\n",
       "0                                                                                  Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n   \n",
       "1                                                                                                                                           Happy new years melody!\\n   \n",
       "2                           PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n   \n",
       "3     URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n   \n",
       "4                                                                                                             I had askd u a question some hours before. Its answer\\n   \n",
       "5        SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\\n   \n",
       "6  Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\\n   \n",
       "7                     Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\\n   \n",
       "\n",
       "   predictions  labels  \n",
       "0            0       0  \n",
       "1            0       0  \n",
       "2            1       1  \n",
       "3            1       1  \n",
       "4            0       0  \n",
       "5            0       1  \n",
       "6            1       0  \n",
       "7            0       1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Make a dataframe with the predictions and the text and the labels\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure model is on the right device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "items_for_manual_review = tokenized_dataset[\"test\"].select(\n",
    "    [0, 1, 22, 31, 43, 292, 448, 487]\n",
    ")\n",
    "\n",
    "results = trainer.predict(items_for_manual_review)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"sms\": [item[\"sms\"] for item in items_for_manual_review],\n",
    "        \"predictions\": results.predictions.argmax(axis=1),\n",
    "        \"labels\": results.label_ids,\n",
    "    }\n",
    ")\n",
    "# Show all the cell\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1e2bb",
   "metadata": {},
   "source": [
    "## Save the LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06d69463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "lora_model.save_pretrained(\"./lora_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b29a4f",
   "metadata": {},
   "source": [
    "# 3. Inference using the PEFT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c5714",
   "metadata": {},
   "source": [
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad996fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type='LORA', auto_mapping=None, base_model_name_or_path='gpt2', revision=None, task_type='SEQ_CLS', inference_mode=True, r=8, target_modules=['c_attn'], lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=True, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from peft import PeftModel, PeftConfig, AutoPeftModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "saved_config = PeftConfig.from_pretrained(\"./lora_output\")\n",
    "print(saved_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5c7cd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "saved_model = AutoPeftModelForSequenceClassification.from_pretrained(\"./lora_output\")\n",
    "saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c42018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e44e9c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sms', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 8\n",
      "})\n",
      "tensor([   56,   929,   986, 14690,   788,   530,  1110,   319, 32803,   356,\n",
      "          460,  1265, 21504, 10247,   290,   474,    72,   323,   259,  1011,\n",
      "         2666,   467,   479,  3301,  2088,   220,   198, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256])\n",
      "{'sms': 'Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n', 'label': tensor(0), 'input_ids': tensor([   56,   929,   986, 14690,   788,   530,  1110,   319, 32803,   356,\n",
      "          460,  1265, 21504, 10247,   290,   474,    72,   323,   259,  1011,\n",
      "         2666,   467,   479,  3301,  2088,   220,   198, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])}\n"
     ]
    }
   ],
   "source": [
    "# Cross check --- \n",
    "#https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification\n",
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Ensure model is on the right device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Handling pad_token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Resize the model embeddings to include the new token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "x = tokenized_dataset[\"test\"].select(\n",
    "    [0, 1, 22, 31, 43, 292, 448, 487]\n",
    ")\n",
    "\n",
    "print(x)\n",
    "\n",
    "for i in x:\n",
    "    print(i['input_ids'])\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56adfed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions ---> \n",
      " [0, 0, 1, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Make a dataframe with the predictions and the text and the labels\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Ensure model is on the right device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "items_for_manual_review = tokenized_dataset[\"test\"].select(\n",
    "    [0, 1, 22, 31, 43, 292, 448, 487]\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "for i in items_for_manual_review:\n",
    "    # Move input tensors to the same device as the model\n",
    "    input_tokens = tokenizer(i['sms'], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**input_tokens).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        predictions.append(predicted_class_id)\n",
    "\n",
    "print(\"Predictions ---> \\n\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a5d5d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy new years melody!\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>tensor(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>tensor(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had askd u a question some hours before. Its answer\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>tensor(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  sms  \\\n",
       "0                                                                                  Yup... Hey then one day on fri we can ask miwa and jiayin take leave go karaoke \\n   \n",
       "1                                                                                                                                           Happy new years melody!\\n   \n",
       "2                           PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08715203652 Identifier Code: 42810 Expires 29/10/0\\n   \n",
       "3     URGENT! We are trying to contact U. Todays draw shows that you have won a £800 prize GUARANTEED. Call 09050003091 from land line. Claim C52. Valid 12hrs only\\n   \n",
       "4                                                                                                             I had askd u a question some hours before. Its answer\\n   \n",
       "5        SMS. ac JSco: Energy is high, but u may not know where 2channel it. 2day ur leadership skills r strong. Psychic? Reply ANS w/question. End? Reply END JSCO\\n   \n",
       "6  Yun ah.the ubi one say if ü wan call by tomorrow.call 67441233 look for irene.ere only got bus8,22,65,61,66,382. Ubi cres,ubi tech park.6ph for 1st 5wkg days.èn\\n   \n",
       "7                     Burger King - Wanna play footy at a top stadium? Get 2 Burger King before 1st Sept and go Large or Super with Coca-Cola and walk out a winner\\n   \n",
       "\n",
       "   predictions     labels  \n",
       "0            0  tensor(0)  \n",
       "1            0  tensor(0)  \n",
       "2            1  tensor(1)  \n",
       "3            1  tensor(1)  \n",
       "4            0  tensor(0)  \n",
       "5            0  tensor(1)  \n",
       "6            0  tensor(0)  \n",
       "7            1  tensor(1)  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"sms\": [item[\"sms\"] for item in items_for_manual_review],\n",
    "        #\"predictions\": results.predictions.argmax(axis=1),\n",
    "        \"predictions\": predictions,\n",
    "        #\"labels\": results.label_ids,\n",
    "        \"labels\": [item[\"label\"] for item in items_for_manual_review],\n",
    "    }\n",
    ")\n",
    "# Show all the cell\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141ca97",
   "metadata": {},
   "source": [
    "# Further Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8649c74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in dataset: tensor([0, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_277/555324270.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  unique_labels = torch.tensor(labels).unique()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "labels = tokenized_dataset[\"test\"][\"label\"]  # Use \"label\" instead of \"labels\"\n",
    "unique_labels = torch.tensor(labels).unique()\n",
    "print(f\"Unique labels in dataset: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e8044",
   "metadata": {},
   "source": [
    "# Evaluate the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "559f6ec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./new_output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Set the learning rate\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Set the per device train batch size and eval batch size\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Evaluate and save the model after each epoch\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataCollatorWithPadding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# trainer.evaluate()\u001b[39;00m\n\u001b[1;32m     44\u001b[0m saved_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Ensure model is on CPU\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:338\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_utils.py:95\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     93\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/random.py:113\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    110\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    111\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 113\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:183\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/random.py:111\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    110\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"0\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    #precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer = Trainer(\n",
    "    model=saved_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./new_output\",\n",
    "        # Set the learning rate\n",
    "        learning_rate=2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.evaluate()\n",
    "\n",
    "saved_model.to(\"cpu\")  # Ensure model is on CPU\n",
    "trainer.model = saved_model  # Update the Trainer's model reference\n",
    "trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55445e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
